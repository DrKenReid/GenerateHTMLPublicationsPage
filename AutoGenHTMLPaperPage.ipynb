{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Auto-Generate Papers Page</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>This little script was written by <a href=\"www.drkenreid.github.com\">Ken Reid</a> for the <a href=\"www.msuqg.github.io\">MSU Quantitative Genetics website</a>. Please give it a star if you use it.</i>\n",
    "\n",
    "This works in 3 steps.\n",
    "\n",
    "<ol>\n",
    "    <li> Set up author names for parameters</li>\n",
    "    <li> Run the scrape, keep only useful information. Removed non-unique entries.</li>\n",
    "    <li> Write the HTML page (this is customised for <a href = \"www.msuqg.github.io\">www.msuqg.github.io</a> but can be easily modified for your own usage). </li>\n",
    "</ol>\n",
    "\n",
    "If you would rather skip most of step 3 and simply copy-paste the scraped and HTML-ized data, that's fine too, see the end of step 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Step One</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the names of authors we wish to list on the page, using the template: <span style=\"font-family: Consolas; font-size: 16px\">\"lastName firstName[Author]\"</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"Gondro Cedric[Author]\",\"Tempelman RJ[Author]\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Step Two</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Thanks to <a href=\"https://gist.github.com/bonzanini/5a4c39e4c02502a8451d\">Bonzanini</a> for the pubmed scraping code utilised (and modified) below. Thanks to <a href = \"https://www.geeksforgeeks.org/python-merging-two-dictionaries/\">geeksforgeeks</a> for the merge function.</i>\n",
    "\n",
    "Import libraries, defined a couple of functions we'll use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you need to install Biopython:\n",
    "# pip install biopython\n",
    "\n",
    "# Full discussion:\n",
    "# https://marcobonzanini.wordpress.com/2015/01/12/searching-pubmed-with-python/\n",
    "import Bio\n",
    "from Bio import Entrez\n",
    "import json\n",
    "import codecs\n",
    "import datetime\n",
    "from datetime import date\n",
    "\n",
    "def search(query):\n",
    "    Entrez.email = 'your.email@example.com'\n",
    "    handle = Entrez.esearch(db='pubmed', \n",
    "                            sort='relevance', \n",
    "                            retmax='20',\n",
    "                            retmode='xml', \n",
    "                            term=query)\n",
    "    results = Entrez.read(handle)\n",
    "    return results\n",
    "\n",
    "def fetch_details(id_list):\n",
    "    ids = ','.join(id_list)\n",
    "    Entrez.email = 'your.email@example.com'\n",
    "    handle = Entrez.efetch(db='pubmed',\n",
    "                           retmode='xml',\n",
    "                           id=ids)\n",
    "    results = Entrez.read(handle)\n",
    "    return results\n",
    "\n",
    "# Python code to merge dict using a single  \n",
    "# expression. \n",
    "def Merge(dict1, dict2): \n",
    "    res = {**dict1, **dict2} \n",
    "    return res \n",
    "\n",
    "# Function used to detect if we are attempting to add a paper\n",
    "# more than once. This can happen if you search for two authors\n",
    "# who have collaborated.\n",
    "def isUnique(results, title):\n",
    "    for result in results:\n",
    "        if result == title:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we scrape PubMed for the above set of authors, then store <i>all</i> results in <span style=\"font-family: Consolas; font-size: 16px\">papers</span>. The results contain all possible information about the paper, except for the actual content. So we first strip the enormous object of 95% of it's data, and retain some chosen information (authors, titles, journals, years). If you wish to save some other information, look inside the commented code and choose a <span style=\"font-family: Consolas; font-size: 16px\">print()</span> command, use it, and explore the <span style=\"font-family: Consolas; font-size: 16px\">papers</span> object for additional data you wish to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    #Create an empty dictionary so we can use the Merge() function\n",
    "    #on the first iteration and every other one, too.\n",
    "    papers = dict() \n",
    "    for name in names:\n",
    "        results = search(name)\n",
    "        id_list = results['IdList']\n",
    "        papers = Merge(papers, fetch_details(id_list))\n",
    "\n",
    "####################################################################################\n",
    "# Test this is running properly by running the following print commands:\n",
    "#\n",
    "# Print each:\n",
    "#for i, paper in enumerate(papers['PubmedArticle']):\n",
    "#   print(\"%d) %s\" % (i+1, paper['MedlineCitation']['Article']['ArticleTitle']))\n",
    "#\n",
    "# Print 1st paper in JSON format:\n",
    "#print(json.dumps(papers['PubmedArticle'][0], indent=2, separators=(',', ':')))\n",
    "####################################################################################\n",
    "\n",
    "# Create a new list to store all unique records, and only the fields we are interested in.\n",
    "reducedData = []\n",
    "for i, paper in enumerate(papers['PubmedArticle']):\n",
    "    # get title\n",
    "    title = \"\\t\\t(%d) %s\" % (i+1, paper['MedlineCitation']['Article']['ArticleTitle'])\n",
    "    if isUnique(reducedData, title):\n",
    "        item = []\n",
    "        item.append(title)\n",
    "        \n",
    "        # get authors\n",
    "        rawAuthors = \"(%d) %s\" % (i+1, paper['MedlineCitation']['Article']['AuthorList'])\n",
    "        splitString = rawAuthors.split(\"Name\")\n",
    "        splitString[0] = \"\"\n",
    "        nameArray = []\n",
    "        comma = True\n",
    "        for chunk in splitString:\n",
    "            chunkFromNameStart = chunk[4:]\n",
    "            #print(chunkFromNameStart)\n",
    "            endOfNameLocation = chunkFromNameStart.find(\"\\'\")\n",
    "            #print(endOfNameLocation)\n",
    "            charsOfInterest = chunkFromNameStart[:endOfNameLocation]\n",
    "            if not charsOfInterest: #if string is empty, ignore and move to next. Some names are missing.\n",
    "                continue\n",
    "            #Ensure format is \"LastName, FirstName; LastName2, FirstName2...\"\n",
    "            if comma == True:\n",
    "                nameArray.append(charsOfInterest + \", \")\n",
    "                comma = False\n",
    "            else:\n",
    "                nameArray.append(charsOfInterest + \"; \")\n",
    "                comma = True\n",
    "        fixedFinalString = nameArray[len(nameArray)-1]\n",
    "        fixedFinalString = fixedFinalString[:-1]\n",
    "        nameArray[len(nameArray)-1] = fixedFinalString #Remove last semicolon\n",
    "        authors = \"\".join(nameArray)\n",
    "        item.append(authors)\n",
    "        \n",
    "        # get date\n",
    "        date = \"(%d) %s\" % (i+1, paper['MedlineCitation']['Article']['ArticleDate'])\n",
    "        locationOfYear = date.find(\"\\'Year\\': \\'\",0,len(date)) + 9\n",
    "        item.append(\", <b>\"+date[locationOfYear:locationOfYear+4]+\"</b>, \") #4 because I doubt this code will be used past the year 9999\n",
    "        \n",
    "        # get journal\n",
    "        journal = \"(%d) %s\" % (i+1, paper['MedlineCitation']['Article']['Journal'])\n",
    "        journalLocation = journal.find(\"\\'Title\\':\",0,len(journal)) + 10\n",
    "        journalLocationEnd = journal.find(\"\\',\",journalLocation,len(journal))\n",
    "        item.append(\"<i>\"+journal[journalLocation:journalLocationEnd]+\"</i>\")\n",
    "        \n",
    "        # add all garnered data to reducedData as a record (string array, really)\n",
    "        reducedData.append(item)\n",
    "        reducedData.append(\"\\n\\n\\t\\t<br><br>\\n\\n\")\n",
    "\n",
    "# Finally, turn into a string for easy use next step.\n",
    "\n",
    "paperData = ''.join(''.join(elems) for elems in reducedData)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Step Three</h2>\n",
    "\n",
    "Finally, we wish to show the information we scraped and cleaned inside an HTML page. By default, this script will look for a file called <span style=\"font-family: Consolas; font-size: 16px\">publications.html</span> in the same file directory. If found, it will then look for, by default, the first \\<\\/h2\\> tag. The list of papers will be inserted after this tag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using codecs library, imported in step 1.\n",
    "file1 = codecs.open(\"publications.html\",'r') \n",
    "webpageCode = file1.read()\n",
    "\n",
    "# 5 because </h2> is 5 characters long. We wish to insert after this.\n",
    "location = webpageCode.find(\"</h2>\",0,len(webpageCode)) + 5\n",
    "\n",
    "# Python cannot modify strings, you have to make a new string instead.\n",
    "prePaperPage = webpageCode[:location] + '\\n\\n<br><br>\\n\\n'\n",
    "midPaperPage = prePaperPage + paperData\n",
    "acknowledgementPaperPage = midPaperPage + \"\\n\\n<br><br>\\n\\n\\t\\t<i>Generated on \"+str(datetime.date.today())+ \" by <a href=https://github.com/DrKenReid/GenerateHTMLPublicationsPage> this tool </a></i>.\"\n",
    "newWebpageCode = acknowledgementPaperPage + webpageCode[location:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great - that's the page generated. Now just to save it as an HTML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "Html_file= open(\"publications.html\",\"w\")\n",
    "Html_file.write(newWebpageCode)\n",
    "Html_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively - if you'd rather simply see the HTML output, run the following cell:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(newWebpageCode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if you'd rather see JUST the paper data, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(paperData)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
